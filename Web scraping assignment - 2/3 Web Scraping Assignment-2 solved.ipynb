{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec4e076",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ‚Äì ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2a2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries reqiured\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fe89214",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbfd6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hiring For Data Analyst I(SQL &amp; Python)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Clario India Pvt Ltd</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring Data analyst with Pharma Background acr...</td>\n",
       "      <td>Hyderabad/Secunderabad, Chennai, Bangalore/Ben...</td>\n",
       "      <td>HCL</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Coordinator | Data Analyst | MS Excel | T...</td>\n",
       "      <td>Bangalore/Bengaluru(Sadashiva Nagar)</td>\n",
       "      <td>Inspiration Manpower Consultancy Pvt. Ltd.</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead Data Analyst - Flipkart Data science group</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0            Hiring For Data Analyst I(SQL & Python)   \n",
       "1                                Senior Data Analyst   \n",
       "2                             Senior Data Analyst II   \n",
       "3                             Senior Data Analyst II   \n",
       "4  Hiring Data analyst with Pharma Background acr...   \n",
       "5  Data Coordinator | Data Analyst | MS Excel | T...   \n",
       "6    Lead Data Analyst - Flipkart Data science group   \n",
       "7                                       Data Analyst   \n",
       "8                                       Data Analyst   \n",
       "9                                       Data Analyst   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Hyderabad/Secunderabad, Chennai, Bangalore/Ben...   \n",
       "5               Bangalore/Bengaluru(Sadashiva Nagar)   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                      Company Experience required  \n",
       "0                        Clario India Pvt Ltd             0-2 Yrs  \n",
       "1                                    Flipkart             3-8 Yrs  \n",
       "2                                    Flipkart             2-4 Yrs  \n",
       "3                                    Flipkart             2-4 Yrs  \n",
       "4                                         HCL            6-11 Yrs  \n",
       "5  Inspiration Manpower Consultancy Pvt. Ltd.             4-7 Yrs  \n",
       "6                                    Flipkart             1-3 Yrs  \n",
       "7                                    Flipkart             1-2 Yrs  \n",
       "8                                    Flipkart             1-2 Yrs  \n",
       "9                                    Flipkart             1-3 Yrs  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "search_job = driver.find_element_by_class_name(\"suggestor-input\")\n",
    "search_job\n",
    "\n",
    "#To write on the search bar\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#finding web element for search location bar using absolute xpath\n",
    "search_locn = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_locn\n",
    "\n",
    "#To write on the search bar\n",
    "search_locn.send_keys(\"Bangalore\")\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#clicking using absolute xpath function\n",
    "search_btn = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_btn\n",
    "\n",
    "#To click on the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Extracting all web elements having job titles\n",
    "title_tags = driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "title_tags\n",
    "\n",
    "#Now the text of job title inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags[:10]:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "job_titles\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Extracting all web elements having job location\n",
    "locn_tags = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "locn_tags\n",
    "\n",
    "#Now the text of job location inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "location = []\n",
    "\n",
    "for i in locn_tags[:10]:\n",
    "    location.append(i.text)\n",
    "\n",
    "location\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Extracting all web elements having company names\n",
    "company_tags = driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "company_tags\n",
    "\n",
    "#Now the text of company names inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags[:10]:\n",
    "    company_names.append(i.text)\n",
    "\n",
    "company_names\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Extracting all web elements having experience using parent tag\n",
    "exp_tag = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "\n",
    "#Now the text of experience inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "experience = []\n",
    "\n",
    "for i in exp_tag[:10]:\n",
    "    experience.append(i.text)\n",
    "\n",
    "experience\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "jobs = pd.DataFrame({'Job_Title':job_titles,'Job_location':location,'Company':company_names,'Experience required':experience})\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae29a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1774d6ae",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9209091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need Data scientists and data engineers - WFH-...</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, M...</td>\n",
       "      <td>Covalense Technologies Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Mobile Premier League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...</td>\n",
       "      <td>HDFC Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computer Vision Python Data Scientist</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...</td>\n",
       "      <td>OceanOfWeb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Minions Ventures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Gojek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior/ Lead Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AmoliTalents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist / Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>open data fabric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru\\n(WFH du...</td>\n",
       "      <td>Concentrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Need Data scientists and data engineers - WFH-...   \n",
       "1                           Principal Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3                                     Data Scientist   \n",
       "4              Computer Vision Python Data Scientist   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                        Senior/ Lead Data Scientist   \n",
       "8                           Data Scientist / Analyst   \n",
       "9    Forecasting Analyst/ Data Scientist (US Client)   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0  Hyderabad/Secunderabad, Bangalore/Bengaluru, M...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2  Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Gurgaon/Gurugram, Bangalore/Bengaluru\\n(WFH du...   \n",
       "\n",
       "                                  Company  \n",
       "0  Covalense Technologies Private Limited  \n",
       "1                   Mobile Premier League  \n",
       "2                               HDFC Bank  \n",
       "3                                   Shell  \n",
       "4                              OceanOfWeb  \n",
       "5                        Minions Ventures  \n",
       "6                                   Gojek  \n",
       "7                            AmoliTalents  \n",
       "8                        open data fabric  \n",
       "9                              Concentrix  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "search_job = driver.find_element_by_class_name(\"suggestor-input\")\n",
    "search_job\n",
    "\n",
    "#To write on the search bar\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#finding web element for search location bar using absolute xpath\n",
    "search_locn = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_locn\n",
    "\n",
    "#To write on the search bar\n",
    "search_locn.send_keys(\"Bangalore\")\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#clicking using absolute xpath function\n",
    "search_btn = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_btn\n",
    "\n",
    "#To click on the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Extracting all web elements having job titles\n",
    "title_tags = driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "title_tags\n",
    "\n",
    "#Now the text of job title inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags[:10]:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "job_titles\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Extracting all web elements having job location\n",
    "locn_tags = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "locn_tags\n",
    "\n",
    "#Now the text of job location inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "location = []\n",
    "\n",
    "for i in locn_tags[:10]:\n",
    "    location.append(i.text)\n",
    "\n",
    "location\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Extracting all web elements having company names\n",
    "company_tags = driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "company_tags\n",
    "\n",
    "#Now the text of company names inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags[:10]:\n",
    "    company_names.append(i.text)\n",
    "\n",
    "company_names\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "jobs = pd.DataFrame({'Job_Title':job_titles,'Job_location':location,'Company':company_names})\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5794e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1f77f34",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47b9110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent Opportunity For Freshers For AI/ML, ...</td>\n",
       "      <td>Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...</td>\n",
       "      <td>NTT Data</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Data Analyst/ Scientist- Fresher Position</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...</td>\n",
       "      <td>Sejal Consulting Hub</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst / Data Scientist / Business Analy...</td>\n",
       "      <td>Noida, New Delhi, Delhi / NCR</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring For Senior Data Scientist-Noida</td>\n",
       "      <td>Noida, New Delhi, Greater Noida</td>\n",
       "      <td>Lumiq.ai</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist _NLP</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR\\n(WFH during ...</td>\n",
       "      <td>EXL</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Only Fresher / Python Data Scientist / Trainee...</td>\n",
       "      <td>Noida, New Delhi, Gurgaon/Gurugram</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Delhi / NCR\\n(WFH during Covid)</td>\n",
       "      <td>Indihire HR Consultants Private Limited</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiring For Data Scientist- Gurgaon/ Noida</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "      <td>Mascot e-services</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Excellent Opportunity For Freshers For AI/ML, ...   \n",
       "1   Junior Data Analyst/ Scientist- Fresher Position   \n",
       "2  Data Analyst / Data Scientist / Business Analy...   \n",
       "3             Hiring For Senior Data Scientist-Noida   \n",
       "4                                Data Scientist _NLP   \n",
       "5                     Data Scientist - MIND Infotech   \n",
       "6                     Data Scientist - MIND Infotech   \n",
       "7  Only Fresher / Python Data Scientist / Trainee...   \n",
       "8                                Lead Data Scientist   \n",
       "9          Hiring For Data Scientist- Gurgaon/ Noida   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0  Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...   \n",
       "1  Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...   \n",
       "2                      Noida, New Delhi, Delhi / NCR   \n",
       "3                    Noida, New Delhi, Greater Noida   \n",
       "4  Bangalore/Bengaluru, Delhi / NCR\\n(WFH during ...   \n",
       "5                                              Noida   \n",
       "6                                              Noida   \n",
       "7                 Noida, New Delhi, Gurgaon/Gurugram   \n",
       "8                    Delhi / NCR\\n(WFH during Covid)   \n",
       "9                            Noida, Gurgaon/Gurugram   \n",
       "\n",
       "                                    Company Experience required  \n",
       "0                                  NTT Data             0-0 Yrs  \n",
       "1                      Sejal Consulting Hub             0-3 Yrs  \n",
       "2                 GABA Consultancy services             0-0 Yrs  \n",
       "3                                  Lumiq.ai             2-6 Yrs  \n",
       "4                                       EXL             3-8 Yrs  \n",
       "5  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED             4-8 Yrs  \n",
       "6  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED             4-8 Yrs  \n",
       "7                 GABA Consultancy services             0-0 Yrs  \n",
       "8   Indihire HR Consultants Private Limited             2-4 Yrs  \n",
       "9                         Mascot e-services             2-4 Yrs  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "search_job = driver.find_element_by_class_name(\"suggestor-input\")\n",
    "search_job\n",
    "\n",
    "#To write on the search bar\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "#clicking using absolute xpath function\n",
    "search_btn = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_btn\n",
    "\n",
    "#To click on the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#To apply filter for location by checking the box\n",
    "location_check = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]')\n",
    "#To click on the box to check\n",
    "location_check.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#To apply filter for location by checking the box\n",
    "salary_check = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]')\n",
    "#To click on the box to check\n",
    "salary_check.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Extracting all web elements having job titles\n",
    "title_tags = driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "title_tags\n",
    "\n",
    "#Now the text of job title inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags[:10]:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "job_titles\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Extracting all web elements having job location\n",
    "locn_tags = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "locn_tags\n",
    "\n",
    "#Now the text of job location inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "location = []\n",
    "\n",
    "for i in locn_tags[:10]:\n",
    "    location.append(i.text)\n",
    "\n",
    "location\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Extracting all web elements having company names\n",
    "company_tags = driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "company_tags\n",
    "\n",
    "#Now the text of company names inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags[:10]:\n",
    "    company_names.append(i.text)\n",
    "\n",
    "company_names\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Extracting all web elements having experience using parent tag\n",
    "exp_tag = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "\n",
    "#Now the text of experience inside the web elements is extracted above\n",
    "#Using For loop to iterate over the tags and extract text from them\n",
    "experience = []\n",
    "\n",
    "for i in exp_tag[:10]:\n",
    "    experience.append(i.text)\n",
    "\n",
    "experience\n",
    "\n",
    "jobs = pd.DataFrame({'Job_Title':job_titles,'Job_location':location,'Company':company_names,'Experience required':experience})\n",
    "jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc547239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18eddd53",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7d5940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 117 120 120 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Discounted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (59)</td>\n",
       "      <td>‚Çπ1,999</td>\n",
       "      <td>62% off</td>\n",
       "      <td>‚Çπ749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>‚Çπ2,500</td>\n",
       "      <td>70% off</td>\n",
       "      <td>‚Çπ749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>‚Çπ1,299</td>\n",
       "      <td>88% off</td>\n",
       "      <td>‚Çπ148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>‚Çπ2,495</td>\n",
       "      <td>91% off</td>\n",
       "      <td>‚Çπ208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>‚Çπ1,599</td>\n",
       "      <td>88% off</td>\n",
       "      <td>‚Çπ179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>‚Çπ1,499</td>\n",
       "      <td>90% off</td>\n",
       "      <td>‚Çπ144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ1,599</td>\n",
       "      <td>89% off</td>\n",
       "      <td>‚Çπ175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection, Mirrored Wayfarer, Retro Square...</td>\n",
       "      <td>‚Çπ899</td>\n",
       "      <td>30% off</td>\n",
       "      <td>‚Çπ629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Polarized, Night Vision, Riding Glasses Sports...</td>\n",
       "      <td>‚Çπ799</td>\n",
       "      <td>31% off</td>\n",
       "      <td>‚Çπ549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Round Sungl...</td>\n",
       "      <td>‚Çπ1,225</td>\n",
       "      <td>79% off</td>\n",
       "      <td>‚Çπ246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                                        Description  \\\n",
       "0   VINCENT CHASE             UV Protection Wayfarer Sunglasses (59)   \n",
       "1   VINCENT CHASE  by Lenskart Polarized, UV Protection Wayfarer ...   \n",
       "2            SRPM             UV Protection Wayfarer Sunglasses (50)   \n",
       "3       Elligator                UV Protection Round Sunglasses (54)   \n",
       "4          PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "..            ...                                                ...   \n",
       "95   Singco India  Gradient, UV Protection Wayfarer Sunglasses (F...   \n",
       "96         PIRASO      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "97       Fastrack  UV Protection, Mirrored Wayfarer, Retro Square...   \n",
       "98       Fastrack  Polarized, Night Vision, Riding Glasses Sports...   \n",
       "99          NuVew  UV Protection, Polarized, Mirrored Round Sungl...   \n",
       "\n",
       "   Original Price Discount Discounted Price  \n",
       "0          ‚Çπ1,999  62% off             ‚Çπ749  \n",
       "1          ‚Çπ2,500  70% off             ‚Çπ749  \n",
       "2          ‚Çπ1,299  88% off             ‚Çπ148  \n",
       "3          ‚Çπ2,495  91% off             ‚Çπ208  \n",
       "4          ‚Çπ1,599  88% off             ‚Çπ179  \n",
       "..            ...      ...              ...  \n",
       "95         ‚Çπ1,499  90% off             ‚Çπ144  \n",
       "96         ‚Çπ1,599  89% off             ‚Çπ175  \n",
       "97           ‚Çπ899  30% off             ‚Çπ629  \n",
       "98           ‚Çπ799  31% off             ‚Çπ549  \n",
       "99         ‚Çπ1,225  79% off             ‚Çπ246  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "url = 'https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#Closing the login popup\n",
    "close_popup = driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "\n",
    "#To click on the close button\n",
    "close_popup.click()\n",
    "\n",
    "time.sleep(3)   #3 seconds wait time for each page\n",
    "\n",
    "#Extracting the elements for search bar\n",
    "search_product = driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_product\n",
    "\n",
    "#To write on the search bar\n",
    "search_product.send_keys(\"sunglasses\")\n",
    "\n",
    "time.sleep(6)\n",
    "\n",
    "#clicking using absolute xpath function\n",
    "search_btn = driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_btn\n",
    "\n",
    "#To click on the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "#Taking the empty lists\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "actual_price=[]\n",
    "discount=[]\n",
    "disc_price=[]\n",
    "\n",
    "#We need to scrap for 100 products and here we will consider the data from first 3 pages\n",
    "#We will take a for loop and scrap data from all the pages\n",
    "for i in range(0,3):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\"):\n",
    "        prod_desc.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3I9_wc']\"):\n",
    "        actual_price.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\"):\n",
    "        discount.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        disc_price.append(j.text)\n",
    "\n",
    "#Path for next page as it changes for every page. We are appending numbers as pages change \n",
    "k=i+1\n",
    "next_page=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"+str(k)\n",
    "driver.get(next_page)\n",
    "time.sleep(2)    \n",
    "\n",
    "#Checking the length of the data scraped\n",
    "print(len(brand),len(prod_desc),len(actual_price),len(discount),len(disc_price))\n",
    "\n",
    "#Creating a new dataframe\n",
    "sunglasses=pd.DataFrame({})\n",
    "sunglasses['Brand']=brand[:100]\n",
    "sunglasses['Description']=prod_desc[:100]\n",
    "sunglasses['Original Price']=actual_price[:100]\n",
    "sunglasses['Discount']=discount[:100]\n",
    "sunglasses['Discounted Price']=disc_price[:100]\n",
    "sunglasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3f265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "238e92e1",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\n",
    "TSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage .\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d054a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 110 110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Full summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>finally an iPhone with very nice battery backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>iPhone is delivered on time. Display is great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>beautiful shining, battery performance is too ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>ü§©ü§©ü§©ü§©ü§©ü§©I loved the Phone.. Every time I pick up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Waste of money!</td>\n",
       "      <td>Mobile is not getting charge.Mobile cannot be ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                 Review  \\\n",
       "0       5              Brilliant   \n",
       "1       5         Simply awesome   \n",
       "2       5    Best in the market!   \n",
       "3       5       Perfect product!   \n",
       "4       5              Fabulous!   \n",
       "..    ...                    ...   \n",
       "95      5      Terrific purchase   \n",
       "96      5      Worth every penny   \n",
       "97      5  Mind-blowing purchase   \n",
       "98      5              Excellent   \n",
       "99      5        Waste of money!   \n",
       "\n",
       "                                         Full summary  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  finally an iPhone with very nice battery backu...  \n",
       "96  iPhone is delivered on time. Display is great ...  \n",
       "97  beautiful shining, battery performance is too ...  \n",
       "98  ü§©ü§©ü§©ü§©ü§©ü§©I loved the Phone.. Every time I pick up...  \n",
       "99  Mobile is not getting charge.Mobile cannot be ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'\n",
    "driver.get(url)\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "#creating empty lists\n",
    "Rating=[]\n",
    "review=[]\n",
    "full_summary=[]\n",
    "\n",
    "#As there are nearly 10 reviews per page, we will check for 11 pages and scrap the required data\n",
    "#Now we will take a for loop and scrap\n",
    "for i in range(0,11):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\"):\n",
    "        Rating.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "        review.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\"):\n",
    "        full_summary.append(j.text)\n",
    "    \n",
    "    #Path for next page as it changes for every page. We are appending numbers as pages change  \n",
    "    k=i+1\n",
    "    next_page=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=\"+str(k) \n",
    "    driver.get(next_page)\n",
    "    time.sleep(2)  #2 seconds wait time for each page  \n",
    "\n",
    "    \n",
    "#Checking the length of the data scraped\n",
    "print(len(Rating),len(review),len(full_summary))    \n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#Creating a new dataframe\n",
    "iphone11=pd.DataFrame({})\n",
    "iphone11['Rating']=Rating[:100]\n",
    "iphone11['Review']=review[:100]\n",
    "iphone11['Full summary']=full_summary[:100]\n",
    "iphone11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d199499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b50298e",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for ‚Äúsneakers‚Äù in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d20d3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 132 160 160 160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Discounted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ2,999</td>\n",
       "      <td>80% off</td>\n",
       "      <td>‚Çπ599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>Kwik FIT casual sneaker shoes and partywear sh...</td>\n",
       "      <td>‚Çπ1,999</td>\n",
       "      <td>82% off</td>\n",
       "      <td>‚Çπ347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>67% off</td>\n",
       "      <td>‚Çπ327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>‚Çπ1,598</td>\n",
       "      <td>68% off</td>\n",
       "      <td>‚Çπ501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TR</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>72% off</td>\n",
       "      <td>‚Çπ272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>SHELBY</td>\n",
       "      <td>Lattest Sneakers Shoe Sneakers For Men</td>\n",
       "      <td>‚Çπ1,299</td>\n",
       "      <td>61% off</td>\n",
       "      <td>‚Çπ499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>D-SNEAKERZ</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ659</td>\n",
       "      <td>62% off</td>\n",
       "      <td>‚Çπ249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Shoes Bank</td>\n",
       "      <td>Combo Pack Of 2 Latest Stylish Casual Shoes fo...</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>75% off</td>\n",
       "      <td>‚Çπ248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>DUNKASTON</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ1,499</td>\n",
       "      <td>82% off</td>\n",
       "      <td>‚Çπ269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>42% off</td>\n",
       "      <td>‚Çπ570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Brand                                        Description  \\\n",
       "0      Numenzo                                   Sneakers For Men   \n",
       "1     KWIK FIT  Kwik FIT casual sneaker shoes and partywear sh...   \n",
       "2     Magnolia                                   Sneakers For Men   \n",
       "3       Chevit  Super Stylish & Trendy Combo Pack of 02 Pairs ...   \n",
       "4           TR                                   Sneakers For Men   \n",
       "..         ...                                                ...   \n",
       "95      SHELBY             Lattest Sneakers Shoe Sneakers For Men   \n",
       "96  D-SNEAKERZ                                   Sneakers For Men   \n",
       "97  Shoes Bank  Combo Pack Of 2 Latest Stylish Casual Shoes fo...   \n",
       "98   DUNKASTON                                   Sneakers For Men   \n",
       "99    RapidBox                                   Sneakers For Men   \n",
       "\n",
       "   Original Price Discount Discounted Price  \n",
       "0          ‚Çπ2,999  80% off             ‚Çπ599  \n",
       "1          ‚Çπ1,999  82% off             ‚Çπ347  \n",
       "2            ‚Çπ999  67% off             ‚Çπ327  \n",
       "3          ‚Çπ1,598  68% off             ‚Çπ501  \n",
       "4            ‚Çπ999  72% off             ‚Çπ272  \n",
       "..            ...      ...              ...  \n",
       "95         ‚Çπ1,299  61% off             ‚Çπ499  \n",
       "96           ‚Çπ659  62% off             ‚Çπ249  \n",
       "97           ‚Çπ999  75% off             ‚Çπ248  \n",
       "98         ‚Çπ1,499  82% off             ‚Çπ269  \n",
       "99           ‚Çπ999  42% off             ‚Çπ570  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "url = 'https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#Closing the login popup\n",
    "close_popup = driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "\n",
    "#To click on the close button\n",
    "close_popup.click()\n",
    "\n",
    "time.sleep(3)   #3 seconds wait time for each page\n",
    "\n",
    "#Extracting the element of search bar\n",
    "search_product = driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_product\n",
    "\n",
    "#To write on the search bar\n",
    "search_product.send_keys(\"sneakers\")\n",
    "\n",
    "time.sleep(6)\n",
    "\n",
    "#clicking using bsolute xpath function\n",
    "search_btn = driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_btn\n",
    "\n",
    "#To click on the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "#Taking the empty lists\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "actual_price=[]\n",
    "discount=[]\n",
    "disc_price=[]\n",
    "\n",
    "#We need to scrap for 100 products and here we will consider the data from first 3 pages\n",
    "#We will take a for loop and scrap data from all the pages\n",
    "for i in range(0,4):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\"):\n",
    "        prod_desc.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3I9_wc']\"):\n",
    "        actual_price.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\"):\n",
    "        discount.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        disc_price.append(j.text)\n",
    "\n",
    "#Path for next page as it changes for every page. We are appending numbers as pages change \n",
    "k=i+1\n",
    "next_page=\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"+str(k)\n",
    "driver.get(next_page)\n",
    "time.sleep(2)    \n",
    "\n",
    "#Checking the length of the data scraped\n",
    "print(len(brand),len(prod_desc),len(actual_price),len(discount),len(disc_price))\n",
    "\n",
    "#Creating a new dataframe\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brand']=brand[:100]\n",
    "sneakers['Description']=prod_desc[:100]\n",
    "sneakers['Original Price']=actual_price[:100]\n",
    "sneakers['Discount']=discount[:100]\n",
    "sneakers['Discounted Price']=disc_price[:100]\n",
    "sneakers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48688810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "946da2d8",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to ‚ÄúRs. 7149 to Rs. 14099 ‚Äù , Color filter to ‚ÄúBlack‚Äù, as shown inthe below image.\n",
    "And then scrape First 100 shoes data you get. The data should include ‚ÄúBrand‚Äù of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7fb5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand name</th>\n",
       "      <th>Short description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Winflo 7 Running Shoes</td>\n",
       "      <td>Rs. 7995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Printed Sneakers</td>\n",
       "      <td>Rs. 9099Rs. 12999(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Leather Driving Shoes</td>\n",
       "      <td>Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Electrify Nitro Running Shoes</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Woven Design Sneakers</td>\n",
       "      <td>Rs. 13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Leather Boots with Buckles</td>\n",
       "      <td>Rs. 8925Rs. 10500(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Pavers England</td>\n",
       "      <td>Men Printed Loafers</td>\n",
       "      <td>Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>High-Top Block Heeled Boots</td>\n",
       "      <td>Rs. 10625Rs. 12500(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Embellished Leather Block Heeled Boots</td>\n",
       "      <td>Rs. 7869Rs. 12900(39% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Women High-Top Heeled Boots</td>\n",
       "      <td>Rs. 12093Rs. 13900(13% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand name                       Short description  \\\n",
       "0             Nike              Men Winflo 7 Running Shoes   \n",
       "1             ALDO                    Men Printed Sneakers   \n",
       "2             ALDO               Men Leather Driving Shoes   \n",
       "3             Puma           Electrify Nitro Running Shoes   \n",
       "4             ALDO               Men Woven Design Sneakers   \n",
       "..             ...                                     ...   \n",
       "95         Saint G              Leather Boots with Buckles   \n",
       "96  Pavers England                     Men Printed Loafers   \n",
       "97         Saint G             High-Top Block Heeled Boots   \n",
       "98         Saint G  Embellished Leather Block Heeled Boots   \n",
       "99         Saint G             Women High-Top Heeled Boots   \n",
       "\n",
       "                          Price  \n",
       "0                      Rs. 7995  \n",
       "1    Rs. 9099Rs. 12999(30% OFF)  \n",
       "2                     Rs. 12999  \n",
       "3                      Rs. 9999  \n",
       "4                     Rs. 13999  \n",
       "..                          ...  \n",
       "95   Rs. 8925Rs. 10500(15% OFF)  \n",
       "96                    Rs. 11999  \n",
       "97  Rs. 10625Rs. 12500(15% OFF)  \n",
       "98   Rs. 7869Rs. 12900(39% OFF)  \n",
       "99  Rs. 12093Rs. 13900(13% OFF)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "url = 'https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "time.sleep(2) #2 seconds wait time\n",
    "\n",
    "#Filtering the color black\n",
    "driver.find_element_by_xpath(\"//span[@data-colorhex='black']\").click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#Filtering the price range Rs. 7149 to Rs. 14099\n",
    "driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]\").click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#Taking the empty lists\n",
    "brand=[]\n",
    "short_desc=[]\n",
    "price=[]\n",
    "\n",
    "#We can see that there are 50 products in one page, and as we need to scrap for 100 products, we need to take 2 pages data\n",
    "for i in range(0,2):\n",
    "    #Finding the tags having the brand name\n",
    "    for j in driver.find_elements_by_xpath(\"//h3[@class='product-brand']\"):\n",
    "        brand.append(j.text) #Extracting text from tags and appending to emptylist\n",
    "    \n",
    "    #Finding the tags having short description of the shoe\n",
    "    for j in driver.find_elements_by_xpath(\"//h4[@class='product-product']\"):\n",
    "        short_desc.append(j.text) #Extracting text from tags and appending to emptylist\n",
    "        \n",
    "    #Finding the tags having the price of the shoe\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='product-price']\"):\n",
    "        price.append(j.text)  #Extracting text from tags and appending to emptylist\n",
    "    driver.find_element_by_class_name('pagination-next').click()   #Button path for moving to next page\n",
    "    time.sleep(2)  \n",
    "    \n",
    "#Checking the length of the list\n",
    "print(len(brand),len(short_desc),len(price))\n",
    "\n",
    "#Creating a new dataframe\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand name']=brand\n",
    "shoes['Short description']=short_desc\n",
    "shoes['Price']=price\n",
    "shoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5858909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eec802b3",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon.\n",
    "Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù and ‚ÄúIntel Core i9‚Äù as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8075be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "time.sleep(2) #2 seconds wait time\n",
    "\n",
    "#Searching laptop in the search bar and clicking the search button\n",
    "search_bar=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "search_bar\n",
    "\n",
    "# Entering ‚ÄúLaptop‚Äù in the search field\n",
    "search_bar.send_keys(\"laptop\")\n",
    "\n",
    "# Clicking on the search button\n",
    "search_btn = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9466053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting CPU Type filter to \"Intel Core i7\"\n",
    "intel_i7 = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[11]/span/a/div')\n",
    "intel_i7.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83879a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) FHD 144Hz, Intel Core i7-11600H 11th Gen, 4GB RTX 3050 Graphics, Gaming Laptop (16GB/512GB SSD/Windows 10/Office 2019/Gray/2.3 Kg), FX566HCB-HN299TS',\n",
       " 'ASUS VivoBook 14 (2021), 14-inch (35.56 cms) FHD, Intel Core i7-1065G7 10th Gen, Thin and Light Laptop (16GB/512GB SSD/Integrated Graphics/Office 2021/Windows 11/Silver/1.6 Kg), X415JA-EK701WS',\n",
       " 'Mi Notebook Ultra 3.2K Resolution Display Intel Core i7-11370H 11th Gen 15.6-inch(39.62 cm) Thin and Light Laptop (16GB/512GB SSD/Iris Xe Graphic/Win 10/MS Office/Backlit KB/Fingerprint Sensor/1.7Kg)',\n",
       " 'ASUS VivoBook 14 (2021), 14-inch (35.56 cms) FHD, Intel Core i7-1065G7 10th Gen, Thin and Light Laptop (16GB/512GB SSD/Integrated Graphics/Office 2021/Windows 11/Silver/1.6 Kg), X415JA-EK701WS',\n",
       " 'ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) FHD 144Hz, Intel Core i7-11600H 11th Gen, 4GB RTX 3050 Graphics, Gaming Laptop (16GB/512GB SSD/Windows 10/Office 2019/Gray/2.3 Kg), FX566HCB-HN299TS',\n",
       " 'LG Gram Intel Evo 11th Gen Core i7 17 inches Ultra-Light Laptop (16 GB RAM, 512 GB SSD, New Windows 11 Home Preload, Iris Xe Graphics, USC -C x 2 (with Power), 1.35 kg, 17Z90P-G.AH85A2, Black)',\n",
       " 'MSI GF75 Thin Gaming, Intel Core i7-10750H, 44cm IPS-Level 144Hz Panel Laptop (8GB/512GB NVMe SSD/Windows 10 Home/Nvidia GeForce GTX 1650/GDDR6 4GB/Black/2.2kg) 10SCXR-654IN',\n",
       " 'ASUS VivoBook K15 OLED (2021), 15.6-inch (39.62 cms) FHD OLED, Intel Core i7-1165G7 11th Gen, Thin and Light Laptop (16GB/512GB SSD/Iris Xe Graphics/Office 2021/Windows 11/Silver/1.8 Kg) K513EA-L712WS',\n",
       " 'HP Pavilion x360 11th Gen Intel Core i7 14 inches FHD, IPS, Convertible Laptop (16 GB RAM/512GB SSD, B&O/Win 11 Home/Backlit keyboard/FPR/ Alexa-Built in/MS Office/Natural Silver/1.52 Kg) -14-dy1013TU',\n",
       " 'Fujitsu UH-X 11th Gen Intel Core i7 13.3‚Äù FHD IPS 400Nits Thin & Light Laptop(16GB/512GB SSD/Windows 11/Office 2021/Iris Xe Graphics/Backlit Kb/Fingerprint Reader/2Yr Warranty/Black/878gms),4ZR1F38024']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping data for laptop titles\n",
    "titles = driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "\n",
    "laptop_titles = []  #empty list\n",
    "for i in titles:\n",
    "        laptop_titles.append(i.text)\n",
    "laptop_titles =laptop_titles[:10]\n",
    "laptop_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0dfc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['89,990',\n",
       " '57,490',\n",
       " '77,499',\n",
       " '57,490',\n",
       " '89,990',\n",
       " '96,999',\n",
       " '70,000',\n",
       " '82,990',\n",
       " '85,790',\n",
       " '84,990']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping data for laptop price\n",
    "price = driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "\n",
    "laptop_price= []  #empty list\n",
    "for i in price:\n",
    "    laptop_price.append(i.text)\n",
    "laptop_price =laptop_price[:10]\n",
    "laptop_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64cd1bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=pa_sp_atf_computers_sr_pg1_1?ie=UTF8&adId=A02130262CGYIQE11FTY0&url=%2FASUS-i7-11600H-RTX-3050-Office-2019-FX566HCB-HN299TS%2Fdp%2FB09PYVKSC7%2Fref%3Dsr_1_1_sspa%3Fcrid%3D1A2G92YQQITAW%26keywords%3Dlaptop%26qid%3D1647427979%26refinements%3Dp_n_feature_thirteen_browse-bin%253A12598163031%26rnid%3D12598141031%26s%3Dcomputers%26sprefix%3D%252Caps%252C285%26sr%3D1-1-spons%26psc%3D1&qualifier=1647427979&id=8238060166086742&widgetName=sp_atf',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=pa_sp_atf_computers_sr_pg1_1?ie=UTF8&adId=A00383291UQK4N429GMLR&url=%2FASUS-VivoBook-i7-1065G7-Integrated-X415JA-EK701WS%2Fdp%2FB09LJ4VLHH%2Fref%3Dsr_1_2_sspa%3Fcrid%3D1A2G92YQQITAW%26keywords%3Dlaptop%26qid%3D1647427979%26refinements%3Dp_n_feature_thirteen_browse-bin%253A12598163031%26rnid%3D12598141031%26s%3Dcomputers%26sprefix%3D%252Caps%252C285%26sr%3D1-2-spons%26psc%3D1&qualifier=1647427979&id=8238060166086742&widgetName=sp_atf',\n",
       " 'https://www.amazon.in/Notebook-resolution-i7-11370H-15-6-inch-Fingerprint/dp/B098XL1SY5/ref=sr_1_3?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-3',\n",
       " 'https://www.amazon.in/ASUS-VivoBook-i7-1065G7-Integrated-X415JA-EK701WS/dp/B09LJ4VLHH/ref=sr_1_4?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-4',\n",
       " 'https://www.amazon.in/ASUS-i7-11600H-RTX-3050-Office-2019-FX566HCB-HN299TS/dp/B09PYVKSC7/ref=sr_1_5?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-5',\n",
       " 'https://www.amazon.in/LG-Ultra-Light-Windows-Graphics-17Z90P-G-AH85A2/dp/B09MHV4CHB/ref=sr_1_6?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-6',\n",
       " 'https://www.amazon.in/MSI-i7-10750H-IPS-Level-Windows-10SCXR-654IN/dp/B093L8QGL7/ref=sr_1_7?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-7',\n",
       " 'https://www.amazon.in/ASUS-VivoBook-15-6-inch-i7-1165G7-K513EA-L712WS/dp/B09NY9BH9V/ref=sr_1_8?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-8',\n",
       " 'https://www.amazon.in/Pavilion-x360-Convertible-Alexa-Built-14-dy1013TU/dp/B09P2RT87H/ref=sr_1_9?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-9',\n",
       " 'https://www.amazon.in/Fujitsu-Graphics-Fingerprint-Warranty-4ZR1F38024/dp/B09HT86ZZQ/ref=sr_1_10?crid=1A2G92YQQITAW&keywords=laptop&qid=1647427979&refinements=p_n_feature_thirteen_browse-bin%3A12598163031&rnid=12598141031&s=computers&sprefix=%2Caps%2C285&sr=1-10']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the tags having the product ratings\n",
    "#First we will collect the urls of all laptops\n",
    "laptop_urls=driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "URL=[]   #Taking an empty list\n",
    "\n",
    "#Appending the url of first 10 laptops to empty list\n",
    "for i in laptop_urls[:10]:\n",
    "    URL.append(i.get_attribute('href'))   #Getting url alone\n",
    "URL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "548c3059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5 out of 5',\n",
       " '4.4 out of 5',\n",
       " '4.3 out of 5',\n",
       " '4.4 out of 5',\n",
       " '5 out of 5',\n",
       " '4.5 out of 5',\n",
       " '4.4 out of 5',\n",
       " '4 out of 5',\n",
       " '4.5 out of 5',\n",
       " '4.4 out of 5']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the ratings of the laptop by using exception as some products dont have any ratings\n",
    "Ratings=[]   #Empty list\n",
    "#Loop for every laptops in the list\n",
    "for url in URL:\n",
    "    driver.get(url)\n",
    "    try:   #Exception handling by using NoSuchElementException\n",
    "        prod_rating=driver.find_element_by_id(\"acrCustomerReviewText\")  #Locating the rating link\n",
    "        prod_rating.click()\n",
    "        rating=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-base\"]') #Locating the rating tags\n",
    "        Ratings.append(rating.text)  #Appending the text from tags to the list\n",
    "    except NoSuchElementException as e:\n",
    "        Ratings.append('No Rating')  #Appending message for products having no ratings\n",
    "\n",
    "Ratings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa177771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "# Checking the length of the scraped data\n",
    "print(len(laptop_titles),len(Ratings),len(laptop_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "356cd639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Title</th>\n",
       "      <th>Laptop Ratings</th>\n",
       "      <th>Laptop Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>5 out of 5</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "      <td>57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mi Notebook Ultra 3.2K Resolution Display Inte...</td>\n",
       "      <td>4.3 out of 5</td>\n",
       "      <td>77,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "      <td>57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>5 out of 5</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>4.5 out of 5</td>\n",
       "      <td>96,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MSI GF75 Thin Gaming, Intel Core i7-10750H, 44...</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "      <td>70,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS VivoBook K15 OLED (2021), 15.6-inch (39.6...</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>82,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion x360 11th Gen Intel Core i7 14 inc...</td>\n",
       "      <td>4.5 out of 5</td>\n",
       "      <td>85,790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fujitsu UH-X 11th Gen Intel Core i7 13.3‚Äù FHD ...</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Laptop Title Laptop Ratings  \\\n",
       "0  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...     5 out of 5   \n",
       "1  ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...   4.4 out of 5   \n",
       "2  Mi Notebook Ultra 3.2K Resolution Display Inte...   4.3 out of 5   \n",
       "3  ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...   4.4 out of 5   \n",
       "4  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...     5 out of 5   \n",
       "5  LG Gram Intel Evo 11th Gen Core i7 17 inches U...   4.5 out of 5   \n",
       "6  MSI GF75 Thin Gaming, Intel Core i7-10750H, 44...   4.4 out of 5   \n",
       "7  ASUS VivoBook K15 OLED (2021), 15.6-inch (39.6...     4 out of 5   \n",
       "8  HP Pavilion x360 11th Gen Intel Core i7 14 inc...   4.5 out of 5   \n",
       "9  Fujitsu UH-X 11th Gen Intel Core i7 13.3‚Äù FHD ...   4.4 out of 5   \n",
       "\n",
       "  Laptop Price  \n",
       "0       89,990  \n",
       "1       57,490  \n",
       "2       77,499  \n",
       "3       57,490  \n",
       "4       89,990  \n",
       "5       96,999  \n",
       "6       70,000  \n",
       "7       82,990  \n",
       "8       85,790  \n",
       "9       84,990  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating DataFrame for the scraped data\n",
    "laptops = pd.DataFrame({})\n",
    "laptops[\"Laptop Title\"] = laptop_titles\n",
    "laptops[\"Laptop Ratings\"] = Ratings\n",
    "laptops[\"Laptop Price\"] = laptop_price\n",
    "laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ae0ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75ecbed3",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of ‚ÄúSearch by Designations, Companies, Skills‚Äù enter\n",
    "‚ÄúData Scientist‚Äù and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of ‚ÄúSearch location‚Äù enter\n",
    "‚ÄúNoida‚Äù and select location ‚ÄúNoida‚Äù.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "764ed918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Extracting the element to click on the jobs option\n",
    "click_jobs = driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[6]')\n",
    "click_jobs\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Clicking on jobs option\n",
    "click_jobs.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b94d3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the search element to enter data scientist designation\n",
    "search_bar = driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div/div/div/div/span/input')\n",
    "\n",
    "search_bar.send_keys(\"Data scientist\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "search_btn = driver.find_element_by_xpath('//*[@id=\"jobs\"]/div[2]/div[1]/div/div/div/button/span')\n",
    "search_btn\n",
    "\n",
    "#clicking on search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "610a7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "\n",
    "#Extracting the element for location button\n",
    "loc_btn = driver.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[1]/p')\n",
    "loc_btn\n",
    "\n",
    "loc_btn.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#Extracting the element fot searching location\n",
    "loc_search = driver.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "loc_search\n",
    "\n",
    "#Entering Noida as preferred location\n",
    "loc_search.send_keys(\"Noida\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#Selecting Noida from drop down list\n",
    "noida = driver.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label')\n",
    "noida\n",
    "\n",
    "#Clicking on noida\n",
    "noida.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9edd6009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMITED\\n ¬∑ \\n3.9\\nbased on 6.4k Reviews',\n",
       " 'NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMITED\\n3.9\\n(6.4k Reviews)',\n",
       " 'HCL Technologies Limited\\n3.8\\n(16.2k Reviews)',\n",
       " 'Optum Global Solutions (India) Private Limited\\n4.1\\n(1.2k Reviews)',\n",
       " 'WSP CONSULTANTS INDIA PRIVATE LIMITED\\n4.2\\n(99 Reviews)',\n",
       " 'Microsoft India (R and D) Pvt Ltd\\n4.2\\n(574 Reviews)',\n",
       " 'HCL Technologies Ltd\\n3.8\\n(16.2k Reviews)',\n",
       " 'Jubilant Foodworks Limited\\n3.9\\n(556 Reviews)',\n",
       " 'HCL Technologies\\n3.8\\n(16.2k Reviews)',\n",
       " 'RATEGAIN TRAVEL TECHNOLOGIES LIMITED\\n3.8\\n(72 Reviews)']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for company name\n",
    "company = driver.find_elements_by_xpath('//div[@class=\"company-info\"]')\n",
    "company\n",
    "\n",
    "name=[] #creating an empty list\n",
    "\n",
    "#Extracting all company names into a list\n",
    "for i in company[:10]:\n",
    "    name.append(i.text)\n",
    "\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bff8334e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['7d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['8d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['18d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['8d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['19d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['16d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['6d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['23d', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['20hr', 'ago'],\n",
       " ['via', 'naukri.com'],\n",
       " ['8d', 'ago'],\n",
       " ['via', 'naukri.com']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for date posted\n",
    "date_posted = driver.find_elements_by_xpath('//span[@class=\"body-small-l\"]')\n",
    "date_posted\n",
    "\n",
    "date=[] #creating an empty list\n",
    "\n",
    "#Extracting all dates posted on into a list\n",
    "for i in date_posted[:20]:\n",
    "    date.append(i.text.split())\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68f47214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['7d', 'ago'],\n",
       " ['8d', 'ago'],\n",
       " ['18d', 'ago'],\n",
       " ['8d', 'ago'],\n",
       " ['19d', 'ago'],\n",
       " ['16d', 'ago'],\n",
       " ['6d', 'ago'],\n",
       " ['23d', 'ago'],\n",
       " ['20hr', 'ago'],\n",
       " ['8d', 'ago']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del date[1::2]\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da7fcd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.9', '3.8', '4.1', '4.2', '4.2', '3.8', '3.9', '3.8', '3.8', '3.7']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for rating\n",
    "ratings = driver.find_elements_by_xpath('//span[@class=\"body-small\"]')\n",
    "ratings\n",
    "\n",
    "rating=[] #creating an empty list\n",
    "\n",
    "#Extracting all ratings into a list\n",
    "for i in ratings[:10]:\n",
    "    rating.append(i.text)\n",
    "\n",
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0ddac96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Date_posted</th>\n",
       "      <th>Rating_of_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMI...</td>\n",
       "      <td>[7d, ago]</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMI...</td>\n",
       "      <td>[8d, ago]</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HCL Technologies Limited\\n3.8\\n(16.2k Reviews)</td>\n",
       "      <td>[18d, ago]</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited...</td>\n",
       "      <td>[8d, ago]</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WSP CONSULTANTS INDIA PRIVATE LIMITED\\n4.2\\n(9...</td>\n",
       "      <td>[19d, ago]</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Microsoft India (R and D) Pvt Ltd\\n4.2\\n(574 R...</td>\n",
       "      <td>[16d, ago]</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HCL Technologies Ltd\\n3.8\\n(16.2k Reviews)</td>\n",
       "      <td>[6d, ago]</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jubilant Foodworks Limited\\n3.9\\n(556 Reviews)</td>\n",
       "      <td>[23d, ago]</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HCL Technologies\\n3.8\\n(16.2k Reviews)</td>\n",
       "      <td>[20hr, ago]</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RATEGAIN TRAVEL TECHNOLOGIES LIMITED\\n3.8\\n(72...</td>\n",
       "      <td>[8d, ago]</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Company  Date_posted  \\\n",
       "0  NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMI...    [7d, ago]   \n",
       "1  NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMI...    [8d, ago]   \n",
       "2     HCL Technologies Limited\\n3.8\\n(16.2k Reviews)   [18d, ago]   \n",
       "3  Optum Global Solutions (India) Private Limited...    [8d, ago]   \n",
       "4  WSP CONSULTANTS INDIA PRIVATE LIMITED\\n4.2\\n(9...   [19d, ago]   \n",
       "5  Microsoft India (R and D) Pvt Ltd\\n4.2\\n(574 R...   [16d, ago]   \n",
       "6         HCL Technologies Ltd\\n3.8\\n(16.2k Reviews)    [6d, ago]   \n",
       "7     Jubilant Foodworks Limited\\n3.9\\n(556 Reviews)   [23d, ago]   \n",
       "8             HCL Technologies\\n3.8\\n(16.2k Reviews)  [20hr, ago]   \n",
       "9  RATEGAIN TRAVEL TECHNOLOGIES LIMITED\\n3.8\\n(72...    [8d, ago]   \n",
       "\n",
       "  Rating_of_company  \n",
       "0               3.9  \n",
       "1               3.8  \n",
       "2               4.1  \n",
       "3               4.2  \n",
       "4               4.2  \n",
       "5               3.8  \n",
       "6               3.9  \n",
       "7               3.8  \n",
       "8               3.8  \n",
       "9               3.7  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe and adding columns to it\n",
    "jobs = pd.DataFrame({'Company':name,'Date_posted':date,'Rating_of_company':rating})\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed454c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02d7ef00",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of ‚ÄúSearch Job Profile‚Äù enters ‚ÄúData Scientist‚Äù and\n",
    "then click on ‚ÄúData Scientist‚Äù.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "323e7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#Getting the website to driver\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Extracting the element to click on the salaries option\n",
    "click_salaries = driver.find_element_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[4]')\n",
    "click_salaries\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Clicking on jobs option\n",
    "click_salaries.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e570172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the search element to enter data scientist designation\n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"jobProfileSearchbox\"]')\n",
    "\n",
    "search_bar.send_keys(\"Data scientist\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "#Extracting element of data scientist designation\n",
    "ds = driver.find_element_by_xpath('//*[@id=\"salaries\"]/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p')\n",
    "ds\n",
    "\n",
    "#clicking on data scientist designation using the extracted xpath\n",
    "ds.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a6a44af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ab', 'Inbev'],\n",
       " ['ZS'],\n",
       " ['Optum'],\n",
       " ['Fractal', 'Analytics'],\n",
       " ['Tiger', 'Analytics'],\n",
       " ['UnitedHealth'],\n",
       " ['Veriz'],\n",
       " ['Ganit', 'Business', 'Solutis'],\n",
       " ['Ericss'],\n",
       " ['Deloitte']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for company name\n",
    "company = driver.find_elements_by_xpath('//div[@class=\"name\"]')\n",
    "company\n",
    "\n",
    "name=[] #creating an empty list\n",
    "\n",
    "#Extracting all company names into a list\n",
    "for i in company[:10]:\n",
    "    name.append(i.text.replace('based','').replace('on','').replace('salaries','').replace('\\n','').split()[:][0:-1])\n",
    "\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6a77a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['based', 'on', '28', 'salaries'],\n",
       " ['based', 'on', '15', 'salaries'],\n",
       " ['based', 'on', '25', 'salaries'],\n",
       " ['based', 'on', '77', 'salaries'],\n",
       " ['based', 'on', '33', 'salaries'],\n",
       " ['based', 'on', '52', 'salaries'],\n",
       " ['based', 'on', '14', 'salaries'],\n",
       " ['based', 'on', '13', 'salaries'],\n",
       " ['based', 'on', '43', 'salaries'],\n",
       " ['based', 'on', '57', 'salaries']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for total salary record\n",
    "total_salary_record = driver.find_elements_by_xpath('//div[@class=\"name\"]')\n",
    "total_salary_record\n",
    "\n",
    "ts=[] #creating an empty list\n",
    "\n",
    "for i in total_salary_record:\n",
    "    ts.append(i.text.split()[:][-4:])\n",
    "    \n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a18bdffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚Çπ 20.3L',\n",
       " '‚Çπ 15.3L',\n",
       " '‚Çπ 15.1L',\n",
       " '‚Çπ 15.1L',\n",
       " '‚Çπ 14.4L',\n",
       " '‚Çπ 13.9L',\n",
       " '‚Çπ 12.7L',\n",
       " '‚Çπ 12.4L',\n",
       " '‚Çπ 11.9L',\n",
       " '‚Çπ 11.7L']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for average salary\n",
    "average_salary = driver.find_elements_by_xpath('//p[@class=\"averageCtc\"]')\n",
    "average_salary\n",
    "\n",
    "avg_sal = [] #creating an empty list\n",
    "\n",
    "for i in average_salary:\n",
    "    avg_sal.append(i.text)\n",
    "    \n",
    "avg_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d19e09c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['‚Çπ', '15.0L'],\n",
       " ['‚Çπ', '9.5L'],\n",
       " ['‚Çπ', '11.0L'],\n",
       " ['‚Çπ', '9.5L'],\n",
       " ['‚Çπ', '8.3L'],\n",
       " ['‚Çπ', '8.3L'],\n",
       " ['‚Çπ', '10.0L'],\n",
       " ['‚Çπ', '8.5L'],\n",
       " ['‚Çπ', '5.8L'],\n",
       " ['‚Çπ', '6.9L']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for minimum salary\n",
    "minimum_salary = driver.find_elements_by_xpath('//div[@class=\"salary-values\"]')\n",
    "minimum_salary\n",
    "\n",
    "mis=[] #creating an empty list\n",
    "\n",
    "for i in minimum_salary:\n",
    "    mis.append(i.text.split()[::][0:2])\n",
    "    \n",
    "mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1d2e93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['25.5L'],\n",
       " ['20.0L'],\n",
       " ['21.3L'],\n",
       " ['22.0L'],\n",
       " ['20.0L'],\n",
       " ['20.5L'],\n",
       " ['21.0L'],\n",
       " ['15.0L'],\n",
       " ['24.0L'],\n",
       " ['23.4L']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for maximum salary\n",
    "maximum_salary = driver.find_elements_by_xpath('//div[@class=\"salary-values\"]')\n",
    "maximum_salary\n",
    "\n",
    "mas=[] #creating an empty list\n",
    "\n",
    "for i in maximum_salary:\n",
    "    mas.append(i.text.split()[::][3:])\n",
    "    \n",
    "mas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e8962194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3-4', 'yrs'],\n",
       " ['2', 'yrs'],\n",
       " ['3-4', 'yrs'],\n",
       " ['2-4', 'yrs'],\n",
       " ['3-4', 'yrs'],\n",
       " ['2-4', 'yrs'],\n",
       " ['4', 'yrs'],\n",
       " ['4', 'yrs'],\n",
       " ['3-4', 'yrs'],\n",
       " ['2-4', 'yrs']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for experience required\n",
    "experience_required = driver.find_elements_by_xpath('//div[@class=\"salaries sbold-list-header\"]')\n",
    "experience_required\n",
    "\n",
    "er=[] #creating an empty list\n",
    "\n",
    "for i in experience_required:\n",
    "    er.append(i.text.replace('Data Scientist\\n','').replace('\\n','').replace('exp','').split()[::][1:])\n",
    "    \n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ebfc9991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>No of salaries based on</th>\n",
       "      <th>Average salary</th>\n",
       "      <th>Minimum salary</th>\n",
       "      <th>Maximum salary</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Ab, Inbev]</td>\n",
       "      <td>[based, on, 28, salaries]</td>\n",
       "      <td>‚Çπ 20.3L</td>\n",
       "      <td>[‚Çπ, 15.0L]</td>\n",
       "      <td>[25.5L]</td>\n",
       "      <td>[3-4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ZS]</td>\n",
       "      <td>[based, on, 15, salaries]</td>\n",
       "      <td>‚Çπ 15.3L</td>\n",
       "      <td>[‚Çπ, 9.5L]</td>\n",
       "      <td>[20.0L]</td>\n",
       "      <td>[2, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Optum]</td>\n",
       "      <td>[based, on, 25, salaries]</td>\n",
       "      <td>‚Çπ 15.1L</td>\n",
       "      <td>[‚Çπ, 11.0L]</td>\n",
       "      <td>[21.3L]</td>\n",
       "      <td>[3-4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Fractal, Analytics]</td>\n",
       "      <td>[based, on, 77, salaries]</td>\n",
       "      <td>‚Çπ 15.1L</td>\n",
       "      <td>[‚Çπ, 9.5L]</td>\n",
       "      <td>[22.0L]</td>\n",
       "      <td>[2-4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Tiger, Analytics]</td>\n",
       "      <td>[based, on, 33, salaries]</td>\n",
       "      <td>‚Çπ 14.4L</td>\n",
       "      <td>[‚Çπ, 8.3L]</td>\n",
       "      <td>[20.0L]</td>\n",
       "      <td>[3-4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[UnitedHealth]</td>\n",
       "      <td>[based, on, 52, salaries]</td>\n",
       "      <td>‚Çπ 13.9L</td>\n",
       "      <td>[‚Çπ, 8.3L]</td>\n",
       "      <td>[20.5L]</td>\n",
       "      <td>[2-4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Veriz]</td>\n",
       "      <td>[based, on, 14, salaries]</td>\n",
       "      <td>‚Çπ 12.7L</td>\n",
       "      <td>[‚Çπ, 10.0L]</td>\n",
       "      <td>[21.0L]</td>\n",
       "      <td>[4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Ganit, Business, Solutis]</td>\n",
       "      <td>[based, on, 13, salaries]</td>\n",
       "      <td>‚Çπ 12.4L</td>\n",
       "      <td>[‚Çπ, 8.5L]</td>\n",
       "      <td>[15.0L]</td>\n",
       "      <td>[4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Ericss]</td>\n",
       "      <td>[based, on, 43, salaries]</td>\n",
       "      <td>‚Çπ 11.9L</td>\n",
       "      <td>[‚Çπ, 5.8L]</td>\n",
       "      <td>[24.0L]</td>\n",
       "      <td>[3-4, yrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Deloitte]</td>\n",
       "      <td>[based, on, 57, salaries]</td>\n",
       "      <td>‚Çπ 11.7L</td>\n",
       "      <td>[‚Çπ, 6.9L]</td>\n",
       "      <td>[23.4L]</td>\n",
       "      <td>[2-4, yrs]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Company    No of salaries based on Average salary  \\\n",
       "0                 [Ab, Inbev]  [based, on, 28, salaries]        ‚Çπ 20.3L   \n",
       "1                        [ZS]  [based, on, 15, salaries]        ‚Çπ 15.3L   \n",
       "2                     [Optum]  [based, on, 25, salaries]        ‚Çπ 15.1L   \n",
       "3        [Fractal, Analytics]  [based, on, 77, salaries]        ‚Çπ 15.1L   \n",
       "4          [Tiger, Analytics]  [based, on, 33, salaries]        ‚Çπ 14.4L   \n",
       "5              [UnitedHealth]  [based, on, 52, salaries]        ‚Çπ 13.9L   \n",
       "6                     [Veriz]  [based, on, 14, salaries]        ‚Çπ 12.7L   \n",
       "7  [Ganit, Business, Solutis]  [based, on, 13, salaries]        ‚Çπ 12.4L   \n",
       "8                    [Ericss]  [based, on, 43, salaries]        ‚Çπ 11.9L   \n",
       "9                  [Deloitte]  [based, on, 57, salaries]        ‚Çπ 11.7L   \n",
       "\n",
       "  Minimum salary Maximum salary Experience required  \n",
       "0     [‚Çπ, 15.0L]        [25.5L]          [3-4, yrs]  \n",
       "1      [‚Çπ, 9.5L]        [20.0L]            [2, yrs]  \n",
       "2     [‚Çπ, 11.0L]        [21.3L]          [3-4, yrs]  \n",
       "3      [‚Çπ, 9.5L]        [22.0L]          [2-4, yrs]  \n",
       "4      [‚Çπ, 8.3L]        [20.0L]          [3-4, yrs]  \n",
       "5      [‚Çπ, 8.3L]        [20.5L]          [2-4, yrs]  \n",
       "6     [‚Çπ, 10.0L]        [21.0L]            [4, yrs]  \n",
       "7      [‚Çπ, 8.5L]        [15.0L]            [4, yrs]  \n",
       "8      [‚Çπ, 5.8L]        [24.0L]          [3-4, yrs]  \n",
       "9      [‚Çπ, 6.9L]        [23.4L]          [2-4, yrs]  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_data = pd.DataFrame({'Company':name,'No of salaries based on':ts,'Average salary':avg_sal,'Minimum salary':mis,'Maximum salary':mas,'Experience required':er})\n",
    "salary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7538740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7725f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
